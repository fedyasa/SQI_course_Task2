# Практическое задание (2)

Ход решения:

1.	Данные были предварительно подготовлены – была применены нормализация: для каждого признака во входных данных из каждого значения вычитается среднее по этому признаку, и разность делится на стандартное отклонение, в результате признак центрируется по нулевому значению и имеет стандартное отклонение, равное единице.

2.	Из-за небольшого кол-ва данных было решено использовать небольшую сеть с двумя промежуточными слоями, имеющими функцию активации ReLU, и линейным последним слоем. Функцией потерь была выбрана среднеквадратичная ошибка (MSE), также был добавлен мониторинг на этапе обучения – средняя абсолютная ошибка (MAE). В качестве оптимизатора было решено выбрать RMSprop:

L1: 256 -> 64

R1: ReLU

L2: 64 -> 64

R2: ReLU

L3: 64 -> 1

Optimizer: RMSprop  
Loss: MSE 
Metrics: MAE  

3.	Также, из-за небольшого набора данных, если бы мы разбили нашу выборку на обучающую и валидационную, валидационный набор получился бы слишком маленьким, что повлекло бы за собой сильные изменения оценки при проверке в зависимости от того, какие данные попадут в валидационный т обучающий наборы. Было решено применить перекрестную проверку по K блокам – доступные данные разделяются на 5 блоков, создаются 5 идентичных моделей, которые обучаются на 4 блоках с оценкой по оставшемуся блоку. По полученным 5 оценкам вычисляется среднее значение, которое принимается как оценка модели.

4.	Выполнено обучение сети с временем, равным 500 эпохам. Для получения информации о качестве обучения в каждую эпоху, было добавлено сохранение оценки проверки (MAE) перед началом эпохи для последующего построения графика. Согласно этому графику, лучшее значение MAE достигается после ~60 эпох, после этого момента начинается переобучение.

5.	После этого было выполнено обучение итоговой версии нейронной сети с 60 эпохами, были получены следующие значения функции потерь и метрики:

Loss: 3.6850
MAE: 1.3307

